{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting from Sound\n",
    "> A Neural Networks project by Aleksander Nikolajev, Kayahan Kaya and Severin Brunner\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter]\n",
    "- sticky_rank: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Gaining information from sounds is a fundamental human ability: We can detect and identify objects just from our hearing as well as estimate the direction and distance of that object. \n",
    "Writing software with the same abilities is a difficult task due to the enormous complexity of audio signals. Applying machine learning, in particular neural networks, is the most promising approach to meet this challenge. \n",
    "In this project, we are researching common methods to deploy neural networks for prediction from sound and are creating our own neural network that is able to extract certain information from audio samples. In particular, we are trying to predict the source of a sound as well as the distance between source and microphone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Audio theory\n",
    "In this section, essential theoretical elements of audio analysis are introduced, which we later utilize for our network.\n",
    "\n",
    "### Pulse-code modulation (PCM)\n",
    "In order to store an analog audio signal in memory, it has to be digitized by applying sampling and quantization. Sampling refers to measuring the signal values at specific timesteps, which transforms the original time-continuous signal into a time-discrete one. Quantization implies mapping the continuous signal values to discrete values in a specific range, e.g. 16 bits.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/b/bf/Pcm.svg \"Sampling and quantization of an analog signal (red) with 4-bit PCM, resulting in a time-discrete and value-discrete signal (blue).\")\n",
    "\n",
    "PCM is a format for storing uncompressed audio signals. It simply contains an array of values that have been produced by sampling and quantizing an analog signal. It has two basic properties:  The sampling rate (how many samples per second were taken) and the bit depth (the number of bits per sample value), which determines the resolution. A typical sampling rate is 44.1 kHz (e.g. CDs), and 16 bits is a common choice for the bit depth.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectograms\n",
    "\n",
    "A spectrogram is a visualiziaton of the frequency spectrum of a signal over time. The frequency spectrum represents the signal strength of the various frequencies present in the signal. It can be calculated by applying a fourier transform to the signal.\n",
    "The spectogram is depicted as a heat map, which means the intensity at a specific frequency and time is expressed through the color.\n",
    "![](clarinette_spectogram.png \"Spectrogram of a recording of a clarinet playing a note. The bottom line is at the frequency of the keynote, the higher lines are the harmonics. The clarinet starts playing at 0.4 seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MFCC (Mel-Frequency Cepstral Coefficients)\n",
    "For audio analysis, it often makes sense to extract certain features from the raw audio signal, like the signal energy or the spectogram. As a feature, the MFCCs represent the entire frequency spectrum compactly with few values (e.g. 40), which approximates the human auditory system more closely. This has proven useful for applications like speech or song recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "\n",
    "In order to train and test the network, a dataset is needed. For the task of identifying objects, there are many datasets available, for example the one from the [2018 Kaggle Freesound competition](https://www.kaggle.com/c/freesound-audio-tagging/data), which contains sounds from 41 different categories such as trumpet or fireworks. We used this dataset for initially creating and testing our sound identification network.\n",
    "\n",
    "However for estimating the distance of the sound source, datasets are scarce. Therefore we decided to create our own.\n",
    "\n",
    "\n",
    "### Creating the dataset\n",
    "TODO: Aleksander, modify it if I got something wrong\n",
    "\n",
    "The [FSD50K dataset](https://zenodo.org/record/4060432) contains sound events from 200 classes. In order to create our dataset for distance prediction, we played the samples from this dataset on a speaker and recorded it with a microphone placed in certain distances to the speaker, i.e. 1 meter, 2 meter and ?? TODO\n",
    "This process introduced some background noise into the samples, which we were attempting to reduce by means of preprocessing.\n",
    "\n",
    "![](dataset_recording_draft.png \"Draft of the recording process. A PC connected to a speaker plays the samples, while a laptop records it with a microphone from a certain distance d. The PC signals the laptop when it starts and stops playing over a socket connection, so the laptop can start and stop recording its samples accordingly.\") \n",
    "\n",
    "\n",
    "### Preprocessing\n",
    "Preprocessing the raw audio material before using it for training is an essential step to improve the accuracy of our network. Common approaches include normalizing the samples as well as reducing background noise. \n",
    "\n",
    "#### Normalization\n",
    "One way to normalize audio signals as part of data preparation is to set the RMS (root mean square) of all audio signals to a fixed value {% fn 2 %}. The RMS of a signal is its effective value, which can be interpreted as the average power output. The intention behind this kind of normalization is to make the network more robust against differences in loudness, enabling the network to better distinguish a loud, remote signal from a quiet, close signal.\n",
    "\n",
    "We set the RMS of each audio signal to one by calculating the RMS over the original signal and then dividing the signal values by the RMS. Employing this normalization technique gave the network accuracy a slight bump.\n",
    "\n",
    "#### Removing background noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hide\n",
    "## The Dataset\n",
    "\n",
    "As a dataset for testing and training, we are using the one provided for the 2018 Kaggle Freesound competition, which is downloadable [here](https://www.kaggle.com/c/freesound-audio-tagging/data).\n",
    "It contains sounds from 41 different categories such as trumpet or fireworks, with 9473 training examples and 1600 test examples. However the samples aren't distributed uniformly over the categories, meaning there's more data for some categories than for others. Also, the amount of manually verified samples varies from category to category.  This might cause the training to become more challenging.\n",
    "\n",
    "![](kaggle_dataset_distribution.png \"The dataset. As it can be seen, only a part of the samples has been manually verified (blue). The amount of samples and the fraction of manually verified samples per category varies between the categories.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network input\n",
    "\n",
    "A major design decision is in which format the input audio signal will be fed to the network. \n",
    "The simplest way would be to emply the raw PCM data, however using features extracted from these data as input instead can provide several advantages. First, the performance will likely be better if meaningful features are chosen. Second, training will be faster if the features represent the raw data more compactly, as the amount of data that the network has to process is smaller. This is especially the case for using MFCCs, as these provide a very compact representation of the frequency spectrum. Another commonly used input format is creating a spectrogram from the raw audio signal and then feeding the spectogram to the network as an image. This allows to employ prevalent image processing techniques to the neural network like convolutional layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using Keras as the deep learning library to construct our network.\n",
    "\n",
    "For audio processing, Librosa is a suitable library. It provides several functions to extract features from audio data, e.g. for creating spectograms, calculating the MFCCs and performing a fourier transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hide\n",
    "\n",
    "Now, we build our network model. \n",
    "The model starts with a convolutional layer followed by ReLU activation and a maxpool layer. Batch normalization is applied by inserting the corresponding layer before the activation function. \n",
    "This structure is repeated 3 more times, then the model ends with a fully connected layer of size 64 with batch normalization. The final output is given by a softmax layer which produces a probability distribution over the 41 classes.\n",
    "\n",
    "As a loss function, we use cross entropy, and the Adam optimizer is used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and future work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source code\n",
    "[include link here]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "{{ 'https://www.kaggle.com/fizzbuzz/beginner-s-guide-to-audio-data/' | fndetail: 1}} USED?\n",
    "\n",
    "{{ 'https://arxiv.org/pdf/2003.04210.pdf' | fndetail: 2}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
