<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Predicting from Sound | fastpages</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Predicting from Sound" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A Neural Networks project by Aleksander Nikolajev, Kayahan Kaya and Severin Brunner" />
<meta property="og:description" content="A Neural Networks project by Aleksander Nikolajev, Kayahan Kaya and Severin Brunner" />
<link rel="canonical" href="https://thegadgeteer.github.io/Predicting-from-Sound-Blog/jupyter/2021/06/14/blog.html" />
<meta property="og:url" content="https://thegadgeteer.github.io/Predicting-from-Sound-Blog/jupyter/2021/06/14/blog.html" />
<meta property="og:site_name" content="fastpages" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-06-14T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"A Neural Networks project by Aleksander Nikolajev, Kayahan Kaya and Severin Brunner","url":"https://thegadgeteer.github.io/Predicting-from-Sound-Blog/jupyter/2021/06/14/blog.html","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://thegadgeteer.github.io/Predicting-from-Sound-Blog/jupyter/2021/06/14/blog.html"},"headline":"Predicting from Sound","dateModified":"2021-06-14T00:00:00-05:00","datePublished":"2021-06-14T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/Predicting-from-Sound-Blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://thegadgeteer.github.io/Predicting-from-Sound-Blog/feed.xml" title="fastpages" /><link rel="shortcut icon" type="image/x-icon" href="/Predicting-from-Sound-Blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/Predicting-from-Sound-Blog/">fastpages</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/Predicting-from-Sound-Blog/about/">About Me</a><a class="page-link" href="/Predicting-from-Sound-Blog/search/">Search</a><a class="page-link" href="/Predicting-from-Sound-Blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Predicting from Sound</h1><p class="page-description">A Neural Networks project by Aleksander Nikolajev, Kayahan Kaya and Severin Brunner</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-06-14T00:00:00-05:00" itemprop="datePublished">
        Jun 14, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      12 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/Predicting-from-Sound-Blog/categories/#jupyter">jupyter</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/TheGadgeteer/Predicting-from-Sound-Blog/tree/master/_notebooks/2021-06-14-blog.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/Predicting-from-Sound-Blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/TheGadgeteer/Predicting-from-Sound-Blog/master?filepath=_notebooks%2F2021-06-14-blog.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/Predicting-from-Sound-Blog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/TheGadgeteer/Predicting-from-Sound-Blog/blob/master/_notebooks/2021-06-14-blog.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/Predicting-from-Sound-Blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#Introduction">Introduction </a></li>
<li class="toc-entry toc-h2"><a href="#Audio-theory">Audio theory </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Pulse-code-modulation-(PCM)">Pulse-code modulation (PCM) </a></li>
<li class="toc-entry toc-h3"><a href="#Spectograms">Spectograms </a></li>
<li class="toc-entry toc-h3"><a href="#MFCC-(Mel-Frequency-Cepstral-Coefficients)">MFCC (Mel-Frequency Cepstral Coefficients) </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#The-Dataset">The Dataset </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Creating-our-own-dataset">Creating our own dataset </a></li>
<li class="toc-entry toc-h3"><a href="#Data-Preprocessing">Data Preprocessing </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Polishing-the-dataset">Polishing the dataset </a></li>
<li class="toc-entry toc-h4"><a href="#Normalization">Normalization </a></li>
<li class="toc-entry toc-h4"><a href="#Removing-background-noise">Removing background noise </a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Creating-the-network">Creating the network </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Model-implementation">Model implementation </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Python-framework">Python framework </a></li>
<li class="toc-entry toc-h4"><a href="#Network-input">Network input </a></li>
<li class="toc-entry toc-h4"><a href="#Classification">Classification </a>
<ul>
<li class="toc-entry toc-h5"><a href="#Tackling-multi-label-classification">Tackling multi-label classification </a></li>
<li class="toc-entry toc-h5"><a href="#Evaluating-the-performance">Evaluating the performance </a></li>
</ul>
</li>
<li class="toc-entry toc-h4"><a href="#Distance">Distance </a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Conclusion-and-future-work">Conclusion and future work </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Contributions-of-the-team-members">Contributions of the team members </a></li>
<li class="toc-entry toc-h3"><a href="#Source-code">Source code </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#References">References </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-06-14-blog.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Introduction">
<a class="anchor" href="#Introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction<a class="anchor-link" href="#Introduction"> </a>
</h2>
<p>Gaining information from sounds is a fundamental human ability. We can detect and identify objects just from our hearing as well as estimate the direction and distance to that object. 
Writing software with the same abilities is a difficult task due to the enormous complexity of audio signals. Applying machine learning, in particular neural networks, is one of the most promising approaches to tackle this challenge. 
In this project, we are researching common methods to deploy neural networks for prediction from sounds and are creating our own neural network that is able to extract certain information from audio samples. In particular, we are trying to predict the source of a sound as well as the distance between its source and the microphone.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Audio-theory">
<a class="anchor" href="#Audio-theory" aria-hidden="true"><span class="octicon octicon-link"></span></a>Audio theory<a class="anchor-link" href="#Audio-theory"> </a>
</h2>
<p>In this section, essential theoretical elements of audio analysis are introduced, which we will later utilize for our network.</p>
<h3 id="Pulse-code-modulation-(PCM)">
<a class="anchor" href="#Pulse-code-modulation-(PCM)" aria-hidden="true"><span class="octicon octicon-link"></span></a>Pulse-code modulation (PCM)<a class="anchor-link" href="#Pulse-code-modulation-(PCM)"> </a>
</h3>
<p>In order to store an analog audio signal in memory, it has to be digitized by applying sampling and quantization. Sampling refers to measuring the signal values at specific timesteps, which transforms the original time-continuous signal into a time-discrete one. Quantization implies mapping the continuous signal values to discrete values in a specific range, e.g. 16 bits.</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/b/bf/Pcm.svg" alt="" title="Figure 1: Sampling and quantization of an analog signal (red) with 4-bit PCM, resulting in a time-discrete and value-discrete signal (blue)."></p>
<p>PCM is a format for storing uncompressed audio signals. It simply stores an array of values that have been produced by sampling and quantizing an analog signal. It has two basic properties:  The sampling rate (how many samples per second were taken) and the bit depth (the number of bits per sample value), which determines the resolution. A typical sampling rate is 44.1 kHz (e.g. CDs), a common choice for the bit depth is 16 bits.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Spectograms">
<a class="anchor" href="#Spectograms" aria-hidden="true"><span class="octicon octicon-link"></span></a>Spectograms<a class="anchor-link" href="#Spectograms"> </a>
</h3>
<p>A spectrogram is a visualiziaton of the frequency spectrum of a signal over time. The frequency spectrum represents the signal strength of the various frequencies present in the signal. It can be calculated by applying a fourier transform to the signal.
The spectogram is depicted as a heat map, which means the intensity at a specific frequency and time is expressed through the color.
<img src="/Predicting-from-Sound-Blog/images/copied_from_nb/clarinette_spectogram.png" alt="" title="Figure 2: Spectrogram of a recording of a clarinet playing a note. The bottom line is at the frequency of the keynote, the higher lines are the harmonics. The clarinet starts playing at 0.4 seconds."></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="MFCC-(Mel-Frequency-Cepstral-Coefficients)">
<a class="anchor" href="#MFCC-(Mel-Frequency-Cepstral-Coefficients)" aria-hidden="true"><span class="octicon octicon-link"></span></a>MFCC (Mel-Frequency Cepstral Coefficients)<a class="anchor-link" href="#MFCC-(Mel-Frequency-Cepstral-Coefficients)"> </a>
</h3>
<p>For audio analysis, it often makes sense to extract certain features from the raw audio signal in order to gain more information about the signal. The MFCCs are such features. They represent the entire frequency spectrum compactly with few values (e.g. 40), which approximates the human auditory system more closely. This has proven useful for applications like speech or song recognition.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Dataset">
<a class="anchor" href="#The-Dataset" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Dataset<a class="anchor-link" href="#The-Dataset"> </a>
</h2>
<p>For our project, we made use of the <a href="https://zenodo.org/record/4060432">FSD50K Zenodo</a> audio dataset, which is commonly used in kaggle competitions. It has over 50k audio samples with 200 classes, with each audio sample corresponding to multiple classes. We utilized this dataset for the classification part of the project.</p>
<h3 id="Creating-our-own-dataset">
<a class="anchor" href="#Creating-our-own-dataset" aria-hidden="true"><span class="octicon octicon-link"></span></a>Creating our own dataset<a class="anchor-link" href="#Creating-our-own-dataset"> </a>
</h3>
<p>While the FSD50K dataset has plenty of sound samples, it cannot be employed for distance prediction on its own. Hence using the FSD50k dataset, we recorded approximately 3000 audio samples from different distances in a room. Note that the room wasn't soundproof, therefore some background noise was inevitably included.</p>
<p>Figure 3 demonstrates the recording process. We placed the microphone in a distance of either 1 meter or 2 meters away from the speaker. To properly record the audio samples, we used a distributed client-server architecture that would notify the microphone client when the speaker would start playing a sample.</p>
<p><img src="/Predicting-from-Sound-Blog/images/copied_from_nb/dataset_recording_draft.png" alt="" title="Figure 3: Draft of the recording process. A PC connected to a speaker plays the samples, while a laptop records it with a microphone from a certain distance d, in our case 1 and 2 meters. The PC signals the laptop when it starts and stops playing each sample over a socket connection, so the laptop can start and stop recording its samples accordingly."></p>
<p>It could be argued that it would have been easier to play and record the sound from the same computer. However this was infeasible, as we didn't have the proper equipment necessary to accomplish this.</p>
<p>After the recording work was done, our final distance dataset contained approximately 3000 audio samples from over 100 different classes.</p>
<h3 id="Data-Preprocessing">
<a class="anchor" href="#Data-Preprocessing" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data Preprocessing<a class="anchor-link" href="#Data-Preprocessing"> </a>
</h3>
<h4 id="Polishing-the-dataset">
<a class="anchor" href="#Polishing-the-dataset" aria-hidden="true"><span class="octicon octicon-link"></span></a>Polishing the dataset<a class="anchor-link" href="#Polishing-the-dataset"> </a>
</h4>
<p>During the creation of the dataset, we noticed that the distributed client-server architecture would sometimes start recording too late, as some audio files were so short that the laptop didn't receive the signal to start recording in time. Because recording again was deemed wasteful of our precious time, we decided to simply filter out the non-existent audio samples. Our dataset was still over 2500 audio samples large even after eliminating the faulty samples.</p>
<p>As mentioned before, a single audio sample could have multiple classes. While this was not an issue with distance prediction, it provided an extra challenge in the classification part of the project. A first step to manage this was to make all audio samples have the same amount of classes.  Figure 4 shows an example of the process we used to accomplish this.</p>
<p><img src="/Predicting-from-Sound-Blog/images/copied_from_nb/figure_prep.png" alt="" title="Figure 4: Adapting the dataset so that the amount of classes is the same for each audio sample"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Normalization">
<a class="anchor" href="#Normalization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Normalization<a class="anchor-link" href="#Normalization"> </a>
</h4>
<p>Normalizing the dataset is a frequently used technique to achieve better training results. In order to normalize raw audio signals, one way is to set the RMS (root mean square) of all signals to a fixed value <sup id="fnref-2" class="footnote-ref"><a href="#fn-2">2</a></sup>. This can be done by calculating the RMS for each audio sample and then dividing all the sample values by the RMS. Normalizing the RMS can be interpreted as ensuring that each audio sample has the same average power output.</p>
<p>Another way is to use the common min/max normalization, which we ended up using in the project due to better results.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Removing-background-noise">
<a class="anchor" href="#Removing-background-noise" aria-hidden="true"><span class="octicon octicon-link"></span></a>Removing background noise<a class="anchor-link" href="#Removing-background-noise"> </a>
</h4>
<p>In order to provide as much meaningful information as possible to the network, any unintended parts of the audio signal should be filtered out. In our case, the goal was to reduce the background noise injected into the samples during the recording process as much as we could. For this task, we tested several common background noise removal approaches for their effectiveness.</p>
<p>The method of short-term energy for noise cancellation is often used for voice detection tasks <sup id="fnref-3" class="footnote-ref"><a href="#fn-3">3</a></sup>. It detects the noisy parts of a signal because they have less energy than the voice parts. The identified noise can then be removed from the signal.</p>
<p>For noise cancellation in vibration signals, autocorrelation has been shown to be a useful tool <sup id="fnref-4" class="footnote-ref"><a href="#fn-4">4</a></sup>. It works because the vibration is correlated to itself, whereas the noise is neither correlated to itself nor the vibration. Therefore repeatedly applying the autocorrelation operation to the signal reduces the noise more and more while the vibration itself is not decreased.</p>
<p>These two concepts however rely on a priori estimates of the signal, e.g. the signal being a voice or a vibration. As our dataset contains lots of very different signals, a priori estimates are impractical for our purposes.</p>
<p>Another method, Adaptive Noise Cancellation (ANC), deals with noise without any of these assumptions. ANC is achieved by introducing a cancelling anti-noise wave through secondary sources, which is generated by a neural network. These secondary sources are interconnected through an electronic system using a specific signal processing algorithm for the particular cancellation scheme. To implement ANC, we wrote an algorithm in Matlab for a single channel feed-forward active noise control system. We then used a sequence of training data to estimate the noise before this noise cancellation setup, and trained the ANC filter with white gaussian noise.</p>
<p>For testing our ANC, we took one of the longer noisy signals from our  dataset and tried to eliminate the noise with the ANC feedforward method. 
Figure 5 shows the difference between the original signal and the noise eliminated signal.</p>
<p><img src="/Predicting-from-Sound-Blog/images/copied_from_nb/ANC_difference.png" alt="" title="Figure 5: Original vs. denoised signal with use of ANC"></p>
<p>Applying this denoising technique indeed resulted in less interference for some samples. However due to the big diversity in our dataset the findings were inconsistent. What worked for one kind of signal effected another kind of signal negatively. Hence we ultimately decided not to deploy any denoising techniques for now and instead let our network deal with the background noise. In future work, handling denoising properly could lead to accuracy improvements.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Creating-the-network">
<a class="anchor" href="#Creating-the-network" aria-hidden="true"><span class="octicon octicon-link"></span></a>Creating the network<a class="anchor-link" href="#Creating-the-network"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Model-implementation">
<a class="anchor" href="#Model-implementation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Model implementation<a class="anchor-link" href="#Model-implementation"> </a>
</h3>
<p>Due to having two different problem statements, we opted on using two different models, one for classification and the other for distance prediction.  In this section, we will discuss the two models that we implemented as well as their performance.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Python-framework">
<a class="anchor" href="#Python-framework" aria-hidden="true"><span class="octicon octicon-link"></span></a>Python framework<a class="anchor-link" href="#Python-framework"> </a>
</h4>
<p>We are using Keras as the deep learning library to construct our network.</p>
<p>For audio processing, Librosa turned out to be a suitable library. It provides several functions to extract features from audio data, e.g. for creating spectograms, calculating the MFCCs or performing a fourier transform.</p>
<p>Other libraries, such as SKlearn and Pandas, helped with the data processing and K-Fold model fitting.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Network-input">
<a class="anchor" href="#Network-input" aria-hidden="true"><span class="octicon octicon-link"></span></a>Network input<a class="anchor-link" href="#Network-input"> </a>
</h4>
<p>The audio source prediction part of this project could have easily boiled down to image classification by using some form of image representation of the audio samples, for example a picture of the raw audio signal or the corresponding spectrogram. Figure 6 shows some of these raw audio inputs.</p>
<p><img src="/Predicting-from-Sound-Blog/images/copied_from_nb/figure_example.png" alt="" title="Figure 6: Some samples of the dataset depicted as pictures of their raw audio signals"></p>
<p>Instead, however, for our first try we opted to extract the MFCC features using Librosa. Librosa returns the MFCC features from a signal over time in an array, which we can then use for classification. As MFCC is a quite compact representation of an audio signal, training is a lot faster due to the network having to process a smaller amount of data.</p>
<p>For the distance part of our project, we decided to try a different approach. We created a spectogram of each audio sample and indeed turned the distance prediction into an image classification problem. There are primarly two reasons for adopting this approach:</p>
<ol>
<li>The previous approach yielded petty results in terms of accuracy.</li>
<li>We wanted to compare feature extraction and image classification approaches.</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Classification">
<a class="anchor" href="#Classification" aria-hidden="true"><span class="octicon octicon-link"></span></a>Classification<a class="anchor-link" href="#Classification"> </a>
</h4>
<p>For the classification, we used a classic Convolution Neural Network (CNN).
This decision was inspired by a popular blog from a kaggle competition called <a href="https://www.kaggle.com/fizzbuzz/beginner-s-guide-to-audio-data/">"Beginners guide to audio data"</a>.</p>
<p>The convolutional network model can be seen in Figure 7.</p>
<p><img src="/Predicting-from-Sound-Blog/images/copied_from_nb/class1.png" alt="" title="Figure 7: The initial CNN model for the classification task"></p>
<p>This model, however, proved to perform very poorly, barely hitting 1% validation accuracy, even though the training accuracy proved to be at 28%. We believe the cause for this was the size of the dataset (2500) being far too small compared to the abundance of classes, leaving little room for error.</p>
<h5 id="Tackling-multi-label-classification">
<a class="anchor" href="#Tackling-multi-label-classification" aria-hidden="true"><span class="octicon octicon-link"></span></a>Tackling multi-label classification<a class="anchor-link" href="#Tackling-multi-label-classification"> </a>
</h5>
<p>In addition to the problems mentioned above, this model wasn't suited to predict multiple labels. Jason Brownlee and his guide "Multi-Label Classification of Satellite Photos of the Amazon Rainforest" <sup id="fnref-5" class="footnote-ref"><a href="#fn-5">5</a></sup> gave us an idea: what we can do is map all n labels to integers and store an n-element vector for each audio file. This vector contains 0 for labels that don't apply to the audio file, and 1 for labels that do. This corresponds to one hot encoding. After utilizing this technique, the predicting process was quite straightforward, resembling an Image Classification task.</p>
<h5 id="Evaluating-the-performance">
<a class="anchor" href="#Evaluating-the-performance" aria-hidden="true"><span class="octicon octicon-link"></span></a>Evaluating the performance<a class="anchor-link" href="#Evaluating-the-performance"> </a>
</h5>
<p>As we are dealing with a multi-class classification task, commonly used performance metrics for binary classification tasks aren't suitable. As an example for binary classifier metrics, the classic F1 score calculates the mean of precision and recall. Here, the precision describes how good the model is at predicting the positive outcome, and the recall quantifies the model's ability to predict positive samples as positive. For predicting multiple classes however, this is inapplicable as there is no clear positive or negative class.</p>
<p>The so-called F-Beta metric overcomes this by first calculating prediction and recall for each class in a one vs rest manner and then averaging them over all classes. A constant Beta is utilized in order to weigh precision and recall differently. We are using the common choice of two for Beta, which makes the recall valued twice as highly as the precision. 
The F-Beta metric is calculated as follows:</p>
<p><em>F-Beta = (1 + Beta^2) x (precision x recall) / (Beta^2 x precision + recall)</em></p>
<p>Figure 8 shows the results as two graphs, the cross entropy loss and F-Beta. This figure explains that the network is quite heavily overfitting the data, however the F-Beta value is close to the ideal, signifying the incredible boost in performance compared to the previous model.</p>
<p><img src="/Predicting-from-Sound-Blog/images/copied_from_nb/second_model.png" alt="" title="Figure 8: Evaluating the training performance of the improved classification network"></p>
<h4 id="Distance">
<a class="anchor" href="#Distance" aria-hidden="true"><span class="octicon octicon-link"></span></a>Distance<a class="anchor-link" href="#Distance"> </a>
</h4>
<p>Regarding the distance prediction model, we were fitting it with the images of all the audio spectograms. As for the model design itself, we used a new concept previously unknown to us: a CRNN (convolutional recurrent neural network).</p>
<p>The idea to use a CRNN for distance classification was proposed by Mariam Yiwere et al. in <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6982911/">"Sound Source Distance Estimation Using Deep Learning: An Image Classification Approach</a>. A CRNN enables the model to learn both the spectral and temporal features and relationships effectively.</p>
<p>The CRNN model we used can be see in Figure 9.
As a loss function, we used cross entropy, and the Adam optimizer was used for training.</p>
<p><img src="/Predicting-from-Sound-Blog/images/copied_from_nb/CRNN.png" alt="" title="Figure 9: The CRNN used for the distance prediction task"></p>
<p>As we only had the time to record two different distances (1 and 2 meters), the hope for this project was that the network at least would predict more accurately than a coin flip. Our hope was fulfilled, since the validation accuracy for this model reached 53%.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Conclusion-and-future-work">
<a class="anchor" href="#Conclusion-and-future-work" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion and future work<a class="anchor-link" href="#Conclusion-and-future-work"> </a>
</h2>
<p>We have created 2 models for two different problem statements - that being audio classification and sound distance prediction. For these tasks, we researched existing models that solve them and adapted them for our purposes. While the results are not great, we believe that the methods we used are intuitive and correct.</p>
<p>For future work, we would continue by attempting to tackle the different kinds of noise cancellation techniques that were researched throughout this project, such as the short-term autocorrellation method.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Contributions-of-the-team-members">
<a class="anchor" href="#Contributions-of-the-team-members" aria-hidden="true"><span class="octicon octicon-link"></span></a>Contributions of the team members<a class="anchor-link" href="#Contributions-of-the-team-members"> </a>
</h3>
<p>Initially, every team member did some research about existing strategies regarding prediction from sound. Later, we split the tasks according to the strengths of each team member:</p>
<ul>
<li>Aleksander was responsible for creating the classification network as well as recording the dataset. </li>
<li>Kayahan managed the project, thereby taking a directive role. He also did extensive research about techniques to eliminate background noise. </li>
<li>Severin created and maintained the blog and undertook debugging tasks.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Source-code">
<a class="anchor" href="#Source-code" aria-hidden="true"><span class="octicon octicon-link"></span></a>Source code<a class="anchor-link" href="#Source-code"> </a>
</h3>
<p><a href="https://github.com/TheGadgeteer/Predicting-from-Sound-Blog/tree/master/Code">Github link</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="References">
<a class="anchor" href="#References" aria-hidden="true"><span class="octicon octicon-link"></span></a>References<a class="anchor-link" href="#References"> </a>
</h2>
<p></p>
<div class="footnotes"><p id="fn-1">1. <a href="https://www.kaggle.com/fizzbuzz/beginner-s-guide-to-audio-data/">https://www.kaggle.com/fizzbuzz/beginner-s-guide-to-audio-data/</a><a href="#fnref-1" class="footnote footnotes">↩</a></p></div>
<p></p>
<div class="footnotes"><p id="fn-2">2. <a href="https://arxiv.org/pdf/2003.04210.pdf">https://arxiv.org/pdf/2003.04210.pdf</a><a href="#fnref-2" class="footnote footnotes">↩</a></p></div>
<p></p>
<div class="footnotes"><p id="fn-3">3. <a href="https://www.researchgate.net/publication/263354982_A_Hierarchical_Framework_Approach_for_Voice_Activity_Detection_and_Speech_Enhancement">https://www.researchgate.net/publication/263354982_A_Hierarchical_Framework_Approach_for_Voice_Activity_Detection_and_Speech_Enhancement</a><a href="#fnref-3" class="footnote footnotes">↩</a></p></div>
<p></p>
<div class="footnotes"><p id="fn-4">4. <a href="https://www.sciencedirect.com/science/article/pii/S2590123020300426">https://www.sciencedirect.com/science/article/pii/S2590123020300426</a><a href="#fnref-4" class="footnote footnotes">↩</a></p></div>
<p></p>
<div class="footnotes"><p id="fn-5">5. <a href="https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-to-classify-satellite-photos-of-the-amazon-rainforest/">https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-to-classify-satellite-photos-of-the-amazon-rainforest/</a><a href="#fnref-5" class="footnote footnotes">↩</a></p></div>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="TheGadgeteer/Predicting-from-Sound-Blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/Predicting-from-Sound-Blog/jupyter/2021/06/14/blog.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/Predicting-from-Sound-Blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/Predicting-from-Sound-Blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/Predicting-from-Sound-Blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastai" title="fastai"><svg class="svg-icon grey"><use xlink:href="/Predicting-from-Sound-Blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/Predicting-from-Sound-Blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
